# AI conversations with Spin & Fermyon Cloud

This sample demonstrates how to leverage Serverless AI and Key-Value Store provided by Spin & Fermyon Cloud to give the LLM model a memory.

## Prerequisites

- Rust must and the `wasm32-wasi` target must be installed on your system.
- The `spin` CLI (here version `2.0.1`) must be installed (see [https://developer.fermyon.com/spin/v2/install](https://developer.fermyon.com/spin/v2/install))
- To deploy the final application to Fermyon Cloud, you need an account at [https://cloud.fermyon.com](https://cloud.fermyon.com) (free tier allows running up to 5 apps)

## App Endpoints

The app exposes the following endpoints:

- `GET /` -> Renders this README as HTML
- `POST /` -> Sends a prompt to the LLM and returns the response along with an `X-ConversationId` header that could be used to send additional questions as part of a bigger conversation (payload must be JSON as shown below)
- `GET /:conversationId` -> Returns the conversation with the given ID (if it exists)

### Prompt payload

The prompt payload is fairly simple. It's just a JSON object with a single `question` property that contains the user prompt to send to the LLM. Here's an example:

```json
{
  "question": "Name 5 countries in Europe"
}
```

### Usage sample with `curl`

The following snippet outlines how to use the app with `curl` (assuming that the app runs locally on port `3000`):

```shell
curl -iX POST \
  -H "Content-Type: application/json" \
  -d '{"question": "Name 5 countries in Europe"}' \
  http://localhost:3000
```

You can grab the `X-ConversationId` header from the response and use it to send additional questions as part of a bigger conversation:

```shell
curl -iX POST \
  -H "Content-Type: application/json" \
  -H "X-ConversationId: 9b8fb387-9ab0-4dff-9233-c3a6f613a407" \
  -d '{"question": "And now 3 in western Europe only"}' \
  http://localhost:3000
```

Finally, you can retrieve the conversation with the given ID:

```shell
curl -iX GET http://localhost:3000/9b8fb387-9ab0-4dff-9233-c3a6f613a407
```

## Running the sample locally

To run the example locally, you must download a variation of `llama2-chat` from huggingface as explained in the [Serverless AI tutorial](https://developer.fermyon.com/spin/v2/ai-sentiment-analysis-api-tutorial#supported-ai-models) provided by Fermyon.

Once you've downloaded the corresponding model and placed it in the `.spin/ai-models` folder, you can build and run the Spin application with:

```shell
# Build the app
spin build

# Run the app locally
spin run
```

You'll quickly recognize that the spinning up the LLM and processing the prompts takes quite some time. This is because the model is quite large and the inference is done on your local machine. To speed things up, we'll can either leverage the `cloud-gpu` proxy, or deploy the application to Fermyon Cloud.

## Cloud-GPU proxy

By using the cloud-gpu proxy, we can still run our application locally. However, we'll consume the LLM running in Fermyon Cloud. To do so, the `cloud-gpu` plugin will deploy a proxy application to our Fermyon Cloud account and provide necessary configuration data to instruct our Spin application to route LLM requests to the proxy.

### Install Cloud-GPU plugin

At the time of writing this app, one could easily install the `cloud-gpu` plugin using the following command:

```shell
# Install the cloud-gpu plugin
spin plugins install \
    -u https://github.com/fermyon/spin-cloud-gpu/releases/download/canary/cloud-gpu.json \
    -y
```

Things may have changed in the meantime. That said, you should definitely check out the corresponding docs at [https://developer.fermyon.com/spin/v2](https://developer.fermyon.com/spin/v2).

### Init Cloud-GPU

Before initializing the cloud-gpu proxy, you must authenticate your `spin` CLI with Fermyon Cloud. To do so, you run the `spin cloud login` command.

With the plugin installed and logged-in with Fermyon Cloud, we can now deploy the proxy application to Fermyon Cloud:

```shell
# init the cloud-gpu proxy
spin cloud-gpu init

```

Grab the `TOML` from the output generated by `spin cloud-gpu init` and store it in a file called `cloud-gpu.toml` in the root of this project.

Finally, we can run the app and leverage the cloud-gpu proxy as shown here:

```shell
# Run the app locally with the cloud-gpu proxy
spin up --runtime-config-file cloud-gpu.toml
```

### Destroy the cloud-gpu proxy

Once you're done with the proxy, you can destroy it with:

```shell
# destroy the cloud-gpu proxy
spin cloud-gpu destroy
```

## Deploying to Fermyon Cloud

Deploying to Fermyon Cloud is super simple. Again it's just a one-liner:

```shell
# Deploy the app to Fermyon Cloud
spin cloud deploy
```
